#!/usr/bin/env python3
"""
Repository Sanitizer
Removes AI fingerprints, emojis, and normalizes content for academic submission.
Only touches comments/markdown/metadata, never alters code semantics.
"""

import os
import re
import yaml
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Set
import difflib

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RepositorySanitizer:
    """Sanitizer for removing AI fingerprints and normalizing content."""
    
    def __init__(self, rules_file="tools/sanitize_rules.yml"):
        self.rules_file = Path(rules_file)
        self.rules = self.load_rules()
        self.changes_made = []
        self.files_processed = 0
        
    def load_rules(self):
        """Load sanitization rules from YAML file."""
        try:
            with open(self.rules_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"Error loading rules file {self.rules_file}: {e}")
            return self.get_default_rules()
    
    def get_default_rules(self):
        """Get default sanitization rules."""
        return {
            'remove_patterns': [
                r'(?i)\b(chatgpt|gpt[- ]?\d|llm|ai[- ]?generated|copilot|cursor|claude)\b',
                r'[\u2190-\u21FF\u2300-\u27BF\u2B00-\u2BFF\u1F300-\u1FAD6]'
            ],
            'replace_phrases': {
                "This was generated by": "",
                "The assistant wrote": "",
                "As an AI": ""
            },
            'files': {
                'include_extensions': ['.py', '.R', '.md', '.ipynb', '.tex', '.yml', '.yaml', '.json', '.toml'],
                'exclude_dirs': ['archive', 'data/raw', '.git', 'venv', '.venv', '__pycache__']
            }
        }
    
    def should_process_file(self, file_path):
        """Check if file should be processed based on rules."""
        file_path = Path(file_path)
        
        # Check extension
        if file_path.suffix not in self.rules['files']['include_extensions']:
            return False
        
        # Check if in excluded directory
        for part in file_path.parts:
            if part in self.rules['files']['exclude_dirs']:
                return False
        
        return True
    
    def sanitize_text(self, text, file_path):
        """Sanitize text content."""
        original_text = text
        sanitized_text = text
        
        # Apply remove patterns
        for pattern in self.rules.get('remove_patterns', []):
            sanitized_text = re.sub(pattern, '', sanitized_text)
        
        # Apply replace phrases
        for old_phrase, new_phrase in self.rules.get('replace_phrases', {}).items():
            sanitized_text = sanitized_text.replace(old_phrase, new_phrase)
        
        # Remove excessive whitespace
        sanitized_text = re.sub(r'\n\s*\n\s*\n', '\n\n', sanitized_text)
        
        # Normalize headers if specified
        if self.rules.get('normalize_headers', False):
            sanitized_text = self.normalize_headers(sanitized_text, file_path)
        
        return sanitized_text
    
    def normalize_headers(self, text, file_path):
        """Normalize file headers."""
        lines = text.split('\n')
        if not lines:
            return text
        
        # Check if first line is a shebang or comment
        if lines[0].startswith('#!') or lines[0].startswith('#'):
            # Keep existing header structure but normalize content
            return text
        
        # Add normalized header for code files
        if file_path.suffix in ['.py', '.R']:
            header = self.generate_header(file_path)
            if not any(line.startswith('# Module:') for line in lines[:5]):
                lines.insert(0, header)
                lines.insert(1, '')
        
        return '\n'.join(lines)
    
    def generate_header(self, file_path):
        """Generate normalized header for file."""
        filename = file_path.name
        purpose = "Research analysis and modeling"
        inputs = "Configuration files and data"
        outputs = "Results and reports"
        
        header_template = self.rules.get('header_template', 
            "# Module: {filename}\n# Purpose: {purpose}\n# Inputs: {inputs}\n# Outputs: {outputs}\n# Notes: Deterministic; no AI-generated commentary.")
        
        return header_template.format(
            filename=filename,
            purpose=purpose,
            inputs=inputs,
            outputs=outputs
        )
    
    def sanitize_notebook(self, file_path):
        """Sanitize Jupyter notebook."""
        try:
            import nbformat
            from nbconvert.preprocessors import ClearOutputPreprocessor
            
            with open(file_path, 'r', encoding='utf-8') as f:
                nb = nbformat.read(f, as_version=4)
            
            # Clear outputs
            preprocessor = ClearOutputPreprocessor()
            nb, _ = preprocessor.preprocess(nb, {})
            
            # Sanitize cell content
            for cell in nb.cells:
                if cell.cell_type == 'markdown':
                    cell.source = self.sanitize_text(cell.source, file_path)
                elif cell.cell_type == 'code':
                    # Sanitize comments in code cells
                    lines = cell.source.split('\n')
                    sanitized_lines = []
                    for line in lines:
                        if line.strip().startswith('#'):
                            sanitized_lines.append(self.sanitize_text(line, file_path))
                        else:
                            sanitized_lines.append(line)
                    cell.source = '\n'.join(sanitized_lines)
            
            # Reset execution counts
            for cell in nb.cells:
                if hasattr(cell, 'execution_count'):
                    cell.execution_count = None
                if hasattr(cell, 'outputs'):
                    cell.outputs = []
            
            return nbformat.writes(nb)
            
        except ImportError:
            logger.warning("nbformat not available, skipping notebook sanitization")
            return None
        except Exception as e:
            logger.error(f"Error sanitizing notebook {file_path}: {e}")
            return None
    
    def process_file(self, file_path, dry_run=True):
        """Process a single file."""
        file_path = Path(file_path)
        
        if not self.should_process_file(file_path):
            return False
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()
        except Exception as e:
            logger.error(f"Error reading {file_path}: {e}")
            return False
        
        # Sanitize content
        if file_path.suffix == '.ipynb':
            sanitized_content = self.sanitize_notebook(file_path)
            if sanitized_content is None:
                return False
        else:
            sanitized_content = self.sanitize_text(original_content, file_path)
        
        # Check if changes were made
        if original_content != sanitized_content:
            self.changes_made.append({
                'file': str(file_path),
                'changes': len(list(difflib.unified_diff(
                    original_content.splitlines(keepends=True),
                    sanitized_content.splitlines(keepends=True),
                    fromfile=str(file_path),
                    tofile=str(file_path)
                )))
            })
            
            if not dry_run:
                # Write sanitized content
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(sanitized_content)
                logger.info(f"Sanitized {file_path}")
            else:
                logger.info(f"Would sanitize {file_path}")
        
        self.files_processed += 1
        return True
    
    def save_diff(self, file_path, original_content, sanitized_content):
        """Save diff to file."""
        diff_path = Path("build/sanitize_diffs") / f"{file_path.name}.diff"
        diff_path.parent.mkdir(parents=True, exist_ok=True)
        
        diff = difflib.unified_diff(
            original_content.splitlines(keepends=True),
            sanitized_content.splitlines(keepends=True),
            fromfile=str(file_path),
            tofile=str(file_path)
        )
        
        with open(diff_path, 'w', encoding='utf-8') as f:
            f.writelines(diff)
    
    def sanitize_repository(self, root_dir=".", dry_run=True):
        """Sanitize entire repository."""
        root_path = Path(root_dir)
        
        logger.info(f"Sanitizing repository: {root_path}")
        logger.info(f"Dry run: {dry_run}")
        
        # Find all files to process
        files_to_process = []
        for file_path in root_path.rglob("*"):
            if file_path.is_file() and self.should_process_file(file_path):
                files_to_process.append(file_path)
        
        logger.info(f"Found {len(files_to_process)} files to process")
        
        # Process files
        for file_path in files_to_process:
            self.process_file(file_path, dry_run)
        
        # Report results
        logger.info(f"Processed {self.files_processed} files")
        logger.info(f"Made changes to {len(self.changes_made)} files")
        
        if self.changes_made:
            logger.info("Files with changes:")
            for change in self.changes_made:
                logger.info(f"  - {change['file']}: {change['changes']} changes")
        
        return len(self.changes_made) > 0

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Sanitize repository for academic submission")
    parser.add_argument("--dry-run", action="store_true", default=True,
                       help="Dry run mode (default: True)")
    parser.add_argument("--apply", action="store_true",
                       help="Apply changes (overrides --dry-run)")
    parser.add_argument("--rules", default="tools/sanitize_rules.yml",
                       help="Rules file (default: tools/sanitize_rules.yml)")
    parser.add_argument("--root-dir", default=".",
                       help="Root directory to sanitize (default: .)")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Determine if dry run
    dry_run = args.dry_run and not args.apply
    
    logger.info("Starting repository sanitization")
    logger.info(f"Rules file: {args.rules}")
    logger.info(f"Root directory: {args.root_dir}")
    logger.info(f"Dry run: {dry_run}")
    
    # Create sanitizer
    sanitizer = RepositorySanitizer(args.rules)
    
    # Sanitize repository
    changes_made = sanitizer.sanitize_repository(args.root_dir, dry_run)
    
    if changes_made:
        if dry_run:
            logger.info("✅ Sanitization plan complete - use --apply to make changes")
            return 0
        else:
            logger.info("✅ Repository sanitized successfully")
            return 0
    else:
        logger.info("✅ No changes needed - repository is already clean")
        return 0

if __name__ == "__main__":
    exit(main())
